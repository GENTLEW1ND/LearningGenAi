{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMR8VFaNSM/7vcdrotknYaD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "711594057a1f459e9d28c6151f032d82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ac9674e7d2ff49189cc63677a8143f1b",
              "IPY_MODEL_5a2fb26cc8aa44c484fb771b4937d8bf",
              "IPY_MODEL_38a4befa74b9466384279f834733967c"
            ],
            "layout": "IPY_MODEL_8fb4374a49094061b733c0333f68973a"
          }
        },
        "ac9674e7d2ff49189cc63677a8143f1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e63e848d095a404f87fcec8864a152d4",
            "placeholder": "​",
            "style": "IPY_MODEL_7cb7f80c79b2443cac86551a21e8749c",
            "value": "config.json: "
          }
        },
        "5a2fb26cc8aa44c484fb771b4937d8bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3dd730846284b1583a1f3c99b0c6bf2",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5b716560bf594f36b66f279e18db3f75",
            "value": 1
          }
        },
        "38a4befa74b9466384279f834733967c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5bfb2efbfd9946548a0dbbb57a160129",
            "placeholder": "​",
            "style": "IPY_MODEL_5aebd036a2ea46b09b8775e784eff58a",
            "value": " 1.58k/? [00:00&lt;00:00, 78.5kB/s]"
          }
        },
        "8fb4374a49094061b733c0333f68973a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e63e848d095a404f87fcec8864a152d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7cb7f80c79b2443cac86551a21e8749c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d3dd730846284b1583a1f3c99b0c6bf2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "5b716560bf594f36b66f279e18db3f75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5bfb2efbfd9946548a0dbbb57a160129": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5aebd036a2ea46b09b8775e784eff58a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b52404fddd2c43ca8b0a53ab6f76fc9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bb493ad43d62483e9086dce4e796750f",
              "IPY_MODEL_8f7165f2972442b7860ba529da035def",
              "IPY_MODEL_16a42d9444ef4bccb86023addc65cd5e"
            ],
            "layout": "IPY_MODEL_400a0dd53aa647e1a2aae8718c978835"
          }
        },
        "bb493ad43d62483e9086dce4e796750f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80d0e4a843454ef3aba1431082591beb",
            "placeholder": "​",
            "style": "IPY_MODEL_579975f53205416e90d4550f25b5bd44",
            "value": "model.safetensors: 100%"
          }
        },
        "8f7165f2972442b7860ba529da035def": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2b5f6771fb94656b61fba3cae3e44a2",
            "max": 1625222120,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3bea5ab4aaec4458907a695198b6c19e",
            "value": 1625222120
          }
        },
        "16a42d9444ef4bccb86023addc65cd5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_229cd375ad9f43db90b9ef7a96c1236f",
            "placeholder": "​",
            "style": "IPY_MODEL_e12dd8e7d74549698ea1cdd696ea5fcf",
            "value": " 1.63G/1.63G [00:27&lt;00:00, 109MB/s]"
          }
        },
        "400a0dd53aa647e1a2aae8718c978835": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80d0e4a843454ef3aba1431082591beb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "579975f53205416e90d4550f25b5bd44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b2b5f6771fb94656b61fba3cae3e44a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3bea5ab4aaec4458907a695198b6c19e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "229cd375ad9f43db90b9ef7a96c1236f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e12dd8e7d74549698ea1cdd696ea5fcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c08e29393ae742e29ac0fabc8a8d0525": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2cf776fc827343788361c9b819f356dc",
              "IPY_MODEL_19f515776b2c4eeaafee6b1972f8b224",
              "IPY_MODEL_1903b25915a945989432326ffe0e9a07"
            ],
            "layout": "IPY_MODEL_1e7d08f863364d4d92323dbdfec5cd91"
          }
        },
        "2cf776fc827343788361c9b819f356dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf9740df6c9d48d4b19b93d2dd94fad2",
            "placeholder": "​",
            "style": "IPY_MODEL_dd607947737d4f069e9c427159a280a6",
            "value": "generation_config.json: 100%"
          }
        },
        "19f515776b2c4eeaafee6b1972f8b224": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e896854670549a2ab1a79abb3900454",
            "max": 363,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_efa282fde1e24b26929b62f1480d9df2",
            "value": 363
          }
        },
        "1903b25915a945989432326ffe0e9a07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f28da7ace0e64bf98c2db8f711b3a7f3",
            "placeholder": "​",
            "style": "IPY_MODEL_ef77d775102d4b4cb97e0afbd85accee",
            "value": " 363/363 [00:00&lt;00:00, 14.0kB/s]"
          }
        },
        "1e7d08f863364d4d92323dbdfec5cd91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf9740df6c9d48d4b19b93d2dd94fad2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd607947737d4f069e9c427159a280a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e896854670549a2ab1a79abb3900454": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efa282fde1e24b26929b62f1480d9df2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f28da7ace0e64bf98c2db8f711b3a7f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef77d775102d4b4cb97e0afbd85accee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ddfeccf187d14433ab563015479b15d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a794abc381664c49b15741aed5f51611",
              "IPY_MODEL_9890e858ab6a442aabd2a7d0b871d3cb",
              "IPY_MODEL_5d5282f6b0814566a5cd7388e706dc2e"
            ],
            "layout": "IPY_MODEL_e8a1461cf2ca456bb9edbede7e53b195"
          }
        },
        "a794abc381664c49b15741aed5f51611": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d1f8fc696684c609a61ef793c5dd91b",
            "placeholder": "​",
            "style": "IPY_MODEL_91639325310a487e91d4bae42205567a",
            "value": "vocab.json: "
          }
        },
        "9890e858ab6a442aabd2a7d0b871d3cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91d3a3afae444f0495d7af992b96075e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_85706d135bc04d4aa9ca84e6a4b82cf1",
            "value": 1
          }
        },
        "5d5282f6b0814566a5cd7388e706dc2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_469b180f2cc045abac50009f2b086303",
            "placeholder": "​",
            "style": "IPY_MODEL_99a4fefb75ec4cf48e7baccb83d85865",
            "value": " 899k/? [00:00&lt;00:00, 16.5MB/s]"
          }
        },
        "e8a1461cf2ca456bb9edbede7e53b195": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d1f8fc696684c609a61ef793c5dd91b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91639325310a487e91d4bae42205567a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "91d3a3afae444f0495d7af992b96075e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "85706d135bc04d4aa9ca84e6a4b82cf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "469b180f2cc045abac50009f2b086303": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99a4fefb75ec4cf48e7baccb83d85865": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85dfdeb1c1294f729abc815e6e42d385": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_68a9ab0a452340a0b8c1e6ddfe4c57f7",
              "IPY_MODEL_2589256763f14f24978ecefa2a558b9f",
              "IPY_MODEL_c6f6c6bb491f472b9d0fccd4dad8e627"
            ],
            "layout": "IPY_MODEL_8c88abdb98ff4e0f9be3ead9041e968a"
          }
        },
        "68a9ab0a452340a0b8c1e6ddfe4c57f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c8c512ed9324d50b0e43a6544dad417",
            "placeholder": "​",
            "style": "IPY_MODEL_da81431298f548ba989715cd1843a495",
            "value": "merges.txt: "
          }
        },
        "2589256763f14f24978ecefa2a558b9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de105f35a82a470797ef21fac3e25b05",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9bb17520991b474295fdc5f2a1060699",
            "value": 1
          }
        },
        "c6f6c6bb491f472b9d0fccd4dad8e627": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7fe3aedce40e41d39f47a68db4fee1a8",
            "placeholder": "​",
            "style": "IPY_MODEL_54082835f80f44a5b4bb063efa19a712",
            "value": " 456k/? [00:00&lt;00:00, 13.1MB/s]"
          }
        },
        "8c88abdb98ff4e0f9be3ead9041e968a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c8c512ed9324d50b0e43a6544dad417": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da81431298f548ba989715cd1843a495": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de105f35a82a470797ef21fac3e25b05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "9bb17520991b474295fdc5f2a1060699": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7fe3aedce40e41d39f47a68db4fee1a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54082835f80f44a5b4bb063efa19a712": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ae731d176ecd41e49a1fe53065472d7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c0c5e29e0e184086b5223c21e76909aa",
              "IPY_MODEL_4c33fdb47e854556b6ae7539d06be0cb",
              "IPY_MODEL_8910b957f7ab4910b157b7d173bb4048"
            ],
            "layout": "IPY_MODEL_c0518b5fd09a4549947018e93304e8f0"
          }
        },
        "c0c5e29e0e184086b5223c21e76909aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa6429fcb8b943cc91b95095bb25a20c",
            "placeholder": "​",
            "style": "IPY_MODEL_ab1eea50d1a6489297fefb4307aaa2af",
            "value": "tokenizer.json: "
          }
        },
        "4c33fdb47e854556b6ae7539d06be0cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4581c7a11e774d6582a06bda44771a9a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_45a2fce2f44b4d88a974aaab8cebfb1b",
            "value": 1
          }
        },
        "8910b957f7ab4910b157b7d173bb4048": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_285d6bcd4569411bbbf01985841c4bb9",
            "placeholder": "​",
            "style": "IPY_MODEL_5dcbb27d2a22459198dbeceb1500fa75",
            "value": " 1.36M/? [00:00&lt;00:00, 20.4MB/s]"
          }
        },
        "c0518b5fd09a4549947018e93304e8f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa6429fcb8b943cc91b95095bb25a20c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab1eea50d1a6489297fefb4307aaa2af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4581c7a11e774d6582a06bda44771a9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "45a2fce2f44b4d88a974aaab8cebfb1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "285d6bcd4569411bbbf01985841c4bb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5dcbb27d2a22459198dbeceb1500fa75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GENTLEW1ND/LearningGenAi/blob/main/ArXiv_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsm71EwEdAHz",
        "outputId": "bd0d2225-e293-4ee3-f85f-fa35cf1b7fcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting arxiv\n",
            "  Downloading arxiv-2.2.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting feedparser~=6.0.10 (from arxiv)\n",
            "  Downloading feedparser-6.0.12-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: requests~=2.32.0 in /usr/local/lib/python3.12/dist-packages (from arxiv) (2.32.4)\n",
            "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests~=2.32.0->arxiv) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests~=2.32.0->arxiv) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests~=2.32.0->arxiv) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests~=2.32.0->arxiv) (2025.8.3)\n",
            "Downloading arxiv-2.2.0-py3-none-any.whl (11 kB)\n",
            "Downloading feedparser-6.0.12-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.5/81.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=b436eb1ccaddcf4f39c6e8e8a15eeca5a85bd5d448d5f52321fb8bccd48171c6\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/f5/1a/23761066dac1d0e8e683e5fdb27e12de53209d05a4a37e6246\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
            "Successfully installed arxiv-2.2.0 feedparser-6.0.12 sgmllib3k-1.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install arxiv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import arxiv\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "0aSvPEyedR9Q"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Query to fetch AI-related papers\n",
        "query = 'ai OR artificial intelligence OR machine learning'\n",
        "search = arxiv.Search(query=query, max_results=10, sort_by=arxiv.SortCriterion.SubmittedDate)\n",
        "\n",
        "#Fetch papers\n",
        "client = arxiv.Client()\n",
        "papers = []\n",
        "for result in client.results(search):\n",
        "  papers.append({\n",
        "      'published': result.published,\n",
        "      'title': result.title,\n",
        "      'abstract': result.summary,\n",
        "      'categories': result.categories\n",
        "  })\n"
      ],
      "metadata": {
        "id": "JE6bJVKCdbVW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # Convert to DataFrame\n",
        "  df = pd.DataFrame(papers)\n",
        "\n",
        "  pd.set_option(\"display.max_colwidth\", None)\n",
        "  df.head(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qy01TovjeWfS",
        "outputId": "a177affd-919d-4fa9-d2e2-f0e798d5096b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  published  \\\n",
              "0 2025-09-18 17:59:58+00:00   \n",
              "1 2025-09-18 17:59:51+00:00   \n",
              "2 2025-09-18 17:59:19+00:00   \n",
              "3 2025-09-18 17:59:16+00:00   \n",
              "4 2025-09-18 17:59:11+00:00   \n",
              "5 2025-09-18 17:58:15+00:00   \n",
              "6 2025-09-18 17:57:07+00:00   \n",
              "7 2025-09-18 17:56:36+00:00   \n",
              "8 2025-09-18 17:56:16+00:00   \n",
              "9 2025-09-18 17:54:01+00:00   \n",
              "\n",
              "                                                                                            title  \\\n",
              "0                            Calibration-Aware Prompt Learning for Medical Vision-Language Models   \n",
              "1  Depth AnyEvent: A Cross-Modal Distillation Paradigm for Event-Based Monocular Depth Estimation   \n",
              "2                Lightweight and Accurate Multi-View Stereo with Confidence-Aware Diffusion Model   \n",
              "3                                     Out-of-Sight Trajectories: Tracking, Fusion, and Prediction   \n",
              "4                                                 Generalizable Geometric Image Caption Synthesis   \n",
              "5                                       Evil Vizier: Vulnerabilities of LLM-Integrated XR Systems   \n",
              "6               Explicit Context-Driven Neural Acoustic Modeling for High-Fidelity RIR Generation   \n",
              "7                                         FlowRL: Matching Reward Distributions for LLM Reasoning   \n",
              "8                                    Fair-GPTQ: Bias-Aware Quantization for Large Language Models   \n",
              "9                       CausalPre: Scalable and Effective Data Pre-processing for Causal Fairness   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          abstract  \\\n",
              "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Medical Vision-Language Models (Med-VLMs) have demonstrated remarkable\\nperformance across diverse medical imaging tasks by leveraging large-scale\\nimage-text pretraining. However, their confidence calibration is largely\\nunexplored, and so remains a significant challenge. As such, miscalibrated\\npredictions can lead to overconfident errors, undermining clinical trust and\\ndecision-making reliability. To address this, we introduce CalibPrompt, the\\nfirst framework to calibrate Med-VLMs during prompt tuning. CalibPrompt\\noptimizes a small set of learnable prompts with carefully designed calibration\\nobjectives under scarce labeled data regime. First, we study a regularizer that\\nattempts to align the smoothed accuracy with the predicted model confidences.\\nSecond, we introduce an angular separation loss to maximize textual feature\\nproximity toward improving the reliability in confidence estimates of\\nmultimodal Med-VLMs. Extensive experiments on four publicly available Med-VLMs\\nand five diverse medical imaging datasets reveal that CalibPrompt consistently\\nimproves calibration without drastically affecting clean accuracy. Our code is\\navailable at https://github.com/iabh1shekbasu/CalibPrompt.   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Event cameras capture sparse, high-temporal-resolution visual information,\\nmaking them particularly suitable for challenging environments with high-speed\\nmotion and strongly varying lighting conditions. However, the lack of large\\ndatasets with dense ground-truth depth annotations hinders learning-based\\nmonocular depth estimation from event data. To address this limitation, we\\npropose a cross-modal distillation paradigm to generate dense proxy labels\\nleveraging a Vision Foundation Model (VFM). Our strategy requires an event\\nstream spatially aligned with RGB frames, a simple setup even available\\noff-the-shelf, and exploits the robustness of large-scale VFMs. Additionally,\\nwe propose to adapt VFMs, either a vanilla one like Depth Anything v2 (DAv2),\\nor deriving from it a novel recurrent architecture to infer depth from\\nmonocular event cameras. We evaluate our approach with synthetic and real-world\\ndatasets, demonstrating that i) our cross-modal paradigm achieves competitive\\nperformance compared to fully supervised methods without requiring expensive\\ndepth annotations, and ii) our VFM-based models achieve state-of-the-art\\nperformance.   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  To reconstruct the 3D geometry from calibrated images, learning-based\\nmulti-view stereo (MVS) methods typically perform multi-view depth estimation\\nand then fuse depth maps into a mesh or point cloud. To improve the\\ncomputational efficiency, many methods initialize a coarse depth map and then\\ngradually refine it in higher resolutions. Recently, diffusion models achieve\\ngreat success in generation tasks. Starting from a random noise, diffusion\\nmodels gradually recover the sample with an iterative denoising process. In\\nthis paper, we propose a novel MVS framework, which introduces diffusion models\\nin MVS. Specifically, we formulate depth refinement as a conditional diffusion\\nprocess. Considering the discriminative characteristic of depth estimation, we\\ndesign a condition encoder to guide the diffusion process. To improve\\nefficiency, we propose a novel diffusion network combining lightweight 2D U-Net\\nand convolutional GRU. Moreover, we propose a novel confidence-based sampling\\nstrategy to adaptively sample depth hypotheses based on the confidence\\nestimated by diffusion model. Based on our novel MVS framework, we propose two\\nnovel MVS methods, DiffMVS and CasDiffMVS. DiffMVS achieves competitive\\nperformance with state-of-the-art efficiency in run-time and GPU memory.\\nCasDiffMVS achieves state-of-the-art performance on DTU, Tanks & Temples and\\nETH3D. Code is available at: https://github.com/cvg/diffmvs.   \n",
              "3  Trajectory prediction is a critical task in computer vision and autonomous\\nsystems, playing a key role in autonomous driving, robotics, surveillance, and\\nvirtual reality. Existing methods often rely on complete and noise-free\\nobservational data, overlooking the challenges associated with out-of-sight\\nobjects and the inherent noise in sensor data caused by limited camera\\ncoverage, obstructions, and the absence of ground truth for denoised\\ntrajectories. These limitations pose safety risks and hinder reliable\\nprediction in real-world scenarios. In this extended work, we present\\nadvancements in Out-of-Sight Trajectory (OST), a novel task that predicts the\\nnoise-free visual trajectories of out-of-sight objects using noisy sensor data.\\nBuilding on our previous research, we broaden the scope of Out-of-Sight\\nTrajectory Prediction (OOSTraj) to include pedestrians and vehicles, extending\\nits applicability to autonomous driving, robotics, surveillance, and virtual\\nreality. Our enhanced Vision-Positioning Denoising Module leverages camera\\ncalibration to establish a vision-positioning mapping, addressing the lack of\\nvisual references, while effectively denoising noisy sensor data in an\\nunsupervised manner. Through extensive evaluations on the Vi-Fi and JRDB\\ndatasets, our approach achieves state-of-the-art performance in both trajectory\\ndenoising and prediction, significantly surpassing previous baselines.\\nAdditionally, we introduce comparisons with traditional denoising methods, such\\nas Kalman filtering, and adapt recent trajectory prediction models to our task,\\nproviding a comprehensive benchmark. This work represents the first initiative\\nto integrate vision-positioning projection for denoising noisy sensor\\ntrajectories of out-of-sight agents, paving the way for future advances. The\\ncode and preprocessed datasets are available at github.com/Hai-chao-Zhang/OST   \n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Multimodal large language models have various practical applications that\\ndemand strong reasoning abilities. Despite recent advancements, these models\\nstill struggle to solve complex geometric problems. A key challenge stems from\\nthe lack of high-quality image-text pair datasets for understanding geometric\\nimages. Furthermore, most template-based data synthesis pipelines typically\\nfail to generalize to questions beyond their predefined templates. In this\\npaper, we bridge this gap by introducing a complementary process of\\nReinforcement Learning with Verifiable Rewards (RLVR) into the data generation\\npipeline. By adopting RLVR to refine captions for geometric images synthesized\\nfrom 50 basic geometric relations and using reward signals derived from\\nmathematical problem-solving tasks, our pipeline successfully captures the key\\nfeatures of geometry problem-solving. This enables better task generalization\\nand yields non-trivial improvements. Furthermore, even in out-of-distribution\\nscenarios, the generated dataset enhances the general reasoning capabilities of\\nmultimodal large language models, yielding accuracy improvements of\\n$2.8\\%\\text{-}4.8\\%$ in statistics, arithmetic, algebraic, and numerical tasks\\nwith non-geometric input images of MathVista and MathVerse, along with\\n$2.4\\%\\text{-}3.9\\%$ improvements in Art, Design, Tech, and Engineering tasks\\nin MMMU.   \n",
              "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Extended reality (XR) applications increasingly integrate Large Language\\nModels (LLMs) to enhance user experience, scene understanding, and even\\ngenerate executable XR content, and are often called \"AI glasses\". Despite\\nthese potential benefits, the integrated XR-LLM pipeline makes XR applications\\nvulnerable to new forms of attacks. In this paper, we analyze LLM-Integated XR\\nsystems in the literature and in practice and categorize them along different\\ndimensions from a systems perspective. Building on this categorization, we\\nidentify a common threat model and demonstrate a series of proof-of-concept\\nattacks on multiple XR platforms that employ various LLM models (Meta Quest 3,\\nMeta Ray-Ban, Android, and Microsoft HoloLens 2 running Llama and GPT models).\\nAlthough these platforms each implement LLM integration differently, they share\\nvulnerabilities where an attacker can modify the public context surrounding a\\nlegitimate LLM query, resulting in erroneous visual or auditory feedback to\\nusers, thus compromising their safety or privacy, sowing confusion, or other\\nharmful effects. To defend against these threats, we discuss mitigation\\nstrategies and best practices for developers, including an initial defense\\nprototype, and call on the community to develop new protection mechanisms to\\nmitigate these risks.   \n",
              "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Realistic sound simulation plays a critical role in many applications. A key\\nelement in sound simulation is the room impulse response (RIR), which\\ncharacterizes how sound propagates from a source to a listener within a given\\nspace. Recent studies have applied neural implicit methods to learn RIR using\\ncontext information collected from the environment, such as scene images.\\nHowever, these approaches do not effectively leverage explicit geometric\\ninformation from the environment. To further exploit the potential of neural\\nimplicit models with direct geometric features, we present Mesh-infused Neural\\nAcoustic Field (MiNAF), which queries a rough room mesh at given locations and\\nextracts distance distributions as an explicit representation of local context.\\nOur approach demonstrates that incorporating explicit local geometric features\\ncan better guide the neural network in generating more accurate RIR\\npredictions. Through comparisons with conventional and state-of-the-art\\nbaseline methods, we show that MiNAF performs competitively across various\\nevaluation metrics. Furthermore, we verify the robustness of MiNAF in datasets\\nwith limited training samples, demonstrating an advance in high-fidelity sound\\nsimulation.   \n",
              "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              We propose FlowRL: matching the full reward distribution via flow balancing\\ninstead of maximizing rewards in large language model (LLM) reinforcement\\nlearning (RL). Recent advanced reasoning models adopt reward-maximizing methods\\n(\\eg, PPO and GRPO), which tend to over-optimize dominant reward signals while\\nneglecting less frequent but valid reasoning paths, thus reducing diversity. In\\ncontrast, we transform scalar rewards into a normalized target distribution\\nusing a learnable partition function, and then minimize the reverse KL\\ndivergence between the policy and the target distribution. We implement this\\nidea as a flow-balanced optimization method that promotes diverse exploration\\nand generalizable reasoning trajectories. We conduct experiments on math and\\ncode reasoning tasks: FlowRL achieves a significant average improvement of\\n$10.0\\%$ over GRPO and $5.1\\%$ over PPO on math benchmarks, and performs\\nconsistently better on code reasoning tasks. These results highlight reward\\ndistribution-matching as a key step toward efficient exploration and diverse\\nreasoning in LLM reinforcement learning.   \n",
              "8                                                                         High memory demands of generative language models have drawn attention to\\nquantization, which reduces computational cost, memory usage, and latency by\\nmapping model weights to lower-precision integers. Approaches such as GPTQ\\neffectively minimize input-weight product errors during quantization; however,\\nrecent empirical studies show that they can increase biased outputs and degrade\\nperformance on fairness benchmarks, and it remains unclear which specific\\nweights cause this issue. In this work, we draw new links between quantization\\nand model fairness by adding explicit group-fairness constraints to the\\nquantization objective and introduce Fair-GPTQ, the first quantization method\\nexplicitly designed to reduce unfairness in large language models. The added\\nconstraints guide the learning of the rounding operation toward less-biased\\ntext generation for protected groups. Specifically, we focus on stereotype\\ngeneration involving occupational bias and discriminatory language spanning\\ngender, race, and religion. Fair-GPTQ has minimal impact on performance,\\npreserving at least 90% of baseline accuracy on zero-shot benchmarks, reduces\\nunfairness relative to a half-precision model, and retains the memory and speed\\nbenefits of 4-bit quantization. We also compare the performance of Fair-GPTQ\\nwith existing debiasing methods and find that it achieves performance on par\\nwith the iterative null-space projection debiasing approach on\\nracial-stereotype benchmarks. Overall, the results validate our theoretical\\nsolution to the quantization problem with a group-bias term, highlight its\\napplicability for reducing group bias at quantization time in generative\\nmodels, and demonstrate that our approach can further be used to analyze\\nchannel- and weight-level contributions to fairness during quantization.   \n",
              "9                                                                                                                                                                                                                                                                                                                                                                                                                                         Causal fairness in databases is crucial to preventing biased and inaccurate\\noutcomes in downstream tasks. While most prior work assumes a known causal\\nmodel, recent efforts relax this assumption by enforcing additional\\nconstraints. However, these approaches often fail to capture broader attribute\\nrelationships that are critical to maintaining utility. This raises a\\nfundamental question: Can we harness the benefits of causal reasoning to design\\nefficient and effective fairness solutions without relying on strong\\nassumptions about the underlying causal model? In this paper, we seek to answer\\nthis question by introducing CausalPre, a scalable and effective\\ncausality-guided data pre-processing framework that guarantees justifiable\\nfairness, a strong causal notion of fairness. CausalPre extracts causally fair\\nrelationships by reformulating the originally complex and computationally\\ninfeasible extraction task into a tailored distribution estimation problem. To\\nensure scalability, CausalPre adopts a carefully crafted variant of\\nlow-dimensional marginal factorization to approximate the joint distribution,\\ncomplemented by a heuristic algorithm that efficiently tackles the associated\\ncomputational challenge. Extensive experiments on benchmark datasets\\ndemonstrate that CausalPre is both effective and scalable, challenging the\\nconventional belief that achieving causal fairness requires trading off\\nrelationship coverage for relaxed model assumptions.   \n",
              "\n",
              "                                                                                                                                                           categories  \n",
              "0                                                                                                                                                             [cs.CV]  \n",
              "1                                                                                                                                                             [cs.CV]  \n",
              "2                                                                                                                                                             [cs.CV]  \n",
              "3  [cs.CV, cs.LG, cs.MA, cs.MM, cs.RO, 68T45, 68U10, 68T07, 68T40, 93C85, 93E11, 62M20, 62M10, 68U05, 94A12, F.2.2; I.2.9; I.2.10; I.4.1; I.4.8; I.4.9; I.5.4; I.3.7]  \n",
              "4                                                                                                                                               [cs.AI, cs.CV, cs.LG]  \n",
              "5                                                                                                                                                             [cs.CR]  \n",
              "6                                                                                                                                               [cs.SD, cs.AI, cs.LG]  \n",
              "7                                                                                                                                               [cs.LG, cs.AI, cs.CL]  \n",
              "8                                                                                                                                                             [cs.CL]  \n",
              "9                                                                                                                                                      [cs.LG, cs.DB]  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d4893d84-8cd7-4f79-a765-ab9d277fbc75\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>published</th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>categories</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2025-09-18 17:59:58+00:00</td>\n",
              "      <td>Calibration-Aware Prompt Learning for Medical Vision-Language Models</td>\n",
              "      <td>Medical Vision-Language Models (Med-VLMs) have demonstrated remarkable\\nperformance across diverse medical imaging tasks by leveraging large-scale\\nimage-text pretraining. However, their confidence calibration is largely\\nunexplored, and so remains a significant challenge. As such, miscalibrated\\npredictions can lead to overconfident errors, undermining clinical trust and\\ndecision-making reliability. To address this, we introduce CalibPrompt, the\\nfirst framework to calibrate Med-VLMs during prompt tuning. CalibPrompt\\noptimizes a small set of learnable prompts with carefully designed calibration\\nobjectives under scarce labeled data regime. First, we study a regularizer that\\nattempts to align the smoothed accuracy with the predicted model confidences.\\nSecond, we introduce an angular separation loss to maximize textual feature\\nproximity toward improving the reliability in confidence estimates of\\nmultimodal Med-VLMs. Extensive experiments on four publicly available Med-VLMs\\nand five diverse medical imaging datasets reveal that CalibPrompt consistently\\nimproves calibration without drastically affecting clean accuracy. Our code is\\navailable at https://github.com/iabh1shekbasu/CalibPrompt.</td>\n",
              "      <td>[cs.CV]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2025-09-18 17:59:51+00:00</td>\n",
              "      <td>Depth AnyEvent: A Cross-Modal Distillation Paradigm for Event-Based Monocular Depth Estimation</td>\n",
              "      <td>Event cameras capture sparse, high-temporal-resolution visual information,\\nmaking them particularly suitable for challenging environments with high-speed\\nmotion and strongly varying lighting conditions. However, the lack of large\\ndatasets with dense ground-truth depth annotations hinders learning-based\\nmonocular depth estimation from event data. To address this limitation, we\\npropose a cross-modal distillation paradigm to generate dense proxy labels\\nleveraging a Vision Foundation Model (VFM). Our strategy requires an event\\nstream spatially aligned with RGB frames, a simple setup even available\\noff-the-shelf, and exploits the robustness of large-scale VFMs. Additionally,\\nwe propose to adapt VFMs, either a vanilla one like Depth Anything v2 (DAv2),\\nor deriving from it a novel recurrent architecture to infer depth from\\nmonocular event cameras. We evaluate our approach with synthetic and real-world\\ndatasets, demonstrating that i) our cross-modal paradigm achieves competitive\\nperformance compared to fully supervised methods without requiring expensive\\ndepth annotations, and ii) our VFM-based models achieve state-of-the-art\\nperformance.</td>\n",
              "      <td>[cs.CV]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2025-09-18 17:59:19+00:00</td>\n",
              "      <td>Lightweight and Accurate Multi-View Stereo with Confidence-Aware Diffusion Model</td>\n",
              "      <td>To reconstruct the 3D geometry from calibrated images, learning-based\\nmulti-view stereo (MVS) methods typically perform multi-view depth estimation\\nand then fuse depth maps into a mesh or point cloud. To improve the\\ncomputational efficiency, many methods initialize a coarse depth map and then\\ngradually refine it in higher resolutions. Recently, diffusion models achieve\\ngreat success in generation tasks. Starting from a random noise, diffusion\\nmodels gradually recover the sample with an iterative denoising process. In\\nthis paper, we propose a novel MVS framework, which introduces diffusion models\\nin MVS. Specifically, we formulate depth refinement as a conditional diffusion\\nprocess. Considering the discriminative characteristic of depth estimation, we\\ndesign a condition encoder to guide the diffusion process. To improve\\nefficiency, we propose a novel diffusion network combining lightweight 2D U-Net\\nand convolutional GRU. Moreover, we propose a novel confidence-based sampling\\nstrategy to adaptively sample depth hypotheses based on the confidence\\nestimated by diffusion model. Based on our novel MVS framework, we propose two\\nnovel MVS methods, DiffMVS and CasDiffMVS. DiffMVS achieves competitive\\nperformance with state-of-the-art efficiency in run-time and GPU memory.\\nCasDiffMVS achieves state-of-the-art performance on DTU, Tanks &amp; Temples and\\nETH3D. Code is available at: https://github.com/cvg/diffmvs.</td>\n",
              "      <td>[cs.CV]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2025-09-18 17:59:16+00:00</td>\n",
              "      <td>Out-of-Sight Trajectories: Tracking, Fusion, and Prediction</td>\n",
              "      <td>Trajectory prediction is a critical task in computer vision and autonomous\\nsystems, playing a key role in autonomous driving, robotics, surveillance, and\\nvirtual reality. Existing methods often rely on complete and noise-free\\nobservational data, overlooking the challenges associated with out-of-sight\\nobjects and the inherent noise in sensor data caused by limited camera\\ncoverage, obstructions, and the absence of ground truth for denoised\\ntrajectories. These limitations pose safety risks and hinder reliable\\nprediction in real-world scenarios. In this extended work, we present\\nadvancements in Out-of-Sight Trajectory (OST), a novel task that predicts the\\nnoise-free visual trajectories of out-of-sight objects using noisy sensor data.\\nBuilding on our previous research, we broaden the scope of Out-of-Sight\\nTrajectory Prediction (OOSTraj) to include pedestrians and vehicles, extending\\nits applicability to autonomous driving, robotics, surveillance, and virtual\\nreality. Our enhanced Vision-Positioning Denoising Module leverages camera\\ncalibration to establish a vision-positioning mapping, addressing the lack of\\nvisual references, while effectively denoising noisy sensor data in an\\nunsupervised manner. Through extensive evaluations on the Vi-Fi and JRDB\\ndatasets, our approach achieves state-of-the-art performance in both trajectory\\ndenoising and prediction, significantly surpassing previous baselines.\\nAdditionally, we introduce comparisons with traditional denoising methods, such\\nas Kalman filtering, and adapt recent trajectory prediction models to our task,\\nproviding a comprehensive benchmark. This work represents the first initiative\\nto integrate vision-positioning projection for denoising noisy sensor\\ntrajectories of out-of-sight agents, paving the way for future advances. The\\ncode and preprocessed datasets are available at github.com/Hai-chao-Zhang/OST</td>\n",
              "      <td>[cs.CV, cs.LG, cs.MA, cs.MM, cs.RO, 68T45, 68U10, 68T07, 68T40, 93C85, 93E11, 62M20, 62M10, 68U05, 94A12, F.2.2; I.2.9; I.2.10; I.4.1; I.4.8; I.4.9; I.5.4; I.3.7]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2025-09-18 17:59:11+00:00</td>\n",
              "      <td>Generalizable Geometric Image Caption Synthesis</td>\n",
              "      <td>Multimodal large language models have various practical applications that\\ndemand strong reasoning abilities. Despite recent advancements, these models\\nstill struggle to solve complex geometric problems. A key challenge stems from\\nthe lack of high-quality image-text pair datasets for understanding geometric\\nimages. Furthermore, most template-based data synthesis pipelines typically\\nfail to generalize to questions beyond their predefined templates. In this\\npaper, we bridge this gap by introducing a complementary process of\\nReinforcement Learning with Verifiable Rewards (RLVR) into the data generation\\npipeline. By adopting RLVR to refine captions for geometric images synthesized\\nfrom 50 basic geometric relations and using reward signals derived from\\nmathematical problem-solving tasks, our pipeline successfully captures the key\\nfeatures of geometry problem-solving. This enables better task generalization\\nand yields non-trivial improvements. Furthermore, even in out-of-distribution\\nscenarios, the generated dataset enhances the general reasoning capabilities of\\nmultimodal large language models, yielding accuracy improvements of\\n$2.8\\%\\text{-}4.8\\%$ in statistics, arithmetic, algebraic, and numerical tasks\\nwith non-geometric input images of MathVista and MathVerse, along with\\n$2.4\\%\\text{-}3.9\\%$ improvements in Art, Design, Tech, and Engineering tasks\\nin MMMU.</td>\n",
              "      <td>[cs.AI, cs.CV, cs.LG]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2025-09-18 17:58:15+00:00</td>\n",
              "      <td>Evil Vizier: Vulnerabilities of LLM-Integrated XR Systems</td>\n",
              "      <td>Extended reality (XR) applications increasingly integrate Large Language\\nModels (LLMs) to enhance user experience, scene understanding, and even\\ngenerate executable XR content, and are often called \"AI glasses\". Despite\\nthese potential benefits, the integrated XR-LLM pipeline makes XR applications\\nvulnerable to new forms of attacks. In this paper, we analyze LLM-Integated XR\\nsystems in the literature and in practice and categorize them along different\\ndimensions from a systems perspective. Building on this categorization, we\\nidentify a common threat model and demonstrate a series of proof-of-concept\\nattacks on multiple XR platforms that employ various LLM models (Meta Quest 3,\\nMeta Ray-Ban, Android, and Microsoft HoloLens 2 running Llama and GPT models).\\nAlthough these platforms each implement LLM integration differently, they share\\nvulnerabilities where an attacker can modify the public context surrounding a\\nlegitimate LLM query, resulting in erroneous visual or auditory feedback to\\nusers, thus compromising their safety or privacy, sowing confusion, or other\\nharmful effects. To defend against these threats, we discuss mitigation\\nstrategies and best practices for developers, including an initial defense\\nprototype, and call on the community to develop new protection mechanisms to\\nmitigate these risks.</td>\n",
              "      <td>[cs.CR]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2025-09-18 17:57:07+00:00</td>\n",
              "      <td>Explicit Context-Driven Neural Acoustic Modeling for High-Fidelity RIR Generation</td>\n",
              "      <td>Realistic sound simulation plays a critical role in many applications. A key\\nelement in sound simulation is the room impulse response (RIR), which\\ncharacterizes how sound propagates from a source to a listener within a given\\nspace. Recent studies have applied neural implicit methods to learn RIR using\\ncontext information collected from the environment, such as scene images.\\nHowever, these approaches do not effectively leverage explicit geometric\\ninformation from the environment. To further exploit the potential of neural\\nimplicit models with direct geometric features, we present Mesh-infused Neural\\nAcoustic Field (MiNAF), which queries a rough room mesh at given locations and\\nextracts distance distributions as an explicit representation of local context.\\nOur approach demonstrates that incorporating explicit local geometric features\\ncan better guide the neural network in generating more accurate RIR\\npredictions. Through comparisons with conventional and state-of-the-art\\nbaseline methods, we show that MiNAF performs competitively across various\\nevaluation metrics. Furthermore, we verify the robustness of MiNAF in datasets\\nwith limited training samples, demonstrating an advance in high-fidelity sound\\nsimulation.</td>\n",
              "      <td>[cs.SD, cs.AI, cs.LG]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2025-09-18 17:56:36+00:00</td>\n",
              "      <td>FlowRL: Matching Reward Distributions for LLM Reasoning</td>\n",
              "      <td>We propose FlowRL: matching the full reward distribution via flow balancing\\ninstead of maximizing rewards in large language model (LLM) reinforcement\\nlearning (RL). Recent advanced reasoning models adopt reward-maximizing methods\\n(\\eg, PPO and GRPO), which tend to over-optimize dominant reward signals while\\nneglecting less frequent but valid reasoning paths, thus reducing diversity. In\\ncontrast, we transform scalar rewards into a normalized target distribution\\nusing a learnable partition function, and then minimize the reverse KL\\ndivergence between the policy and the target distribution. We implement this\\nidea as a flow-balanced optimization method that promotes diverse exploration\\nand generalizable reasoning trajectories. We conduct experiments on math and\\ncode reasoning tasks: FlowRL achieves a significant average improvement of\\n$10.0\\%$ over GRPO and $5.1\\%$ over PPO on math benchmarks, and performs\\nconsistently better on code reasoning tasks. These results highlight reward\\ndistribution-matching as a key step toward efficient exploration and diverse\\nreasoning in LLM reinforcement learning.</td>\n",
              "      <td>[cs.LG, cs.AI, cs.CL]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2025-09-18 17:56:16+00:00</td>\n",
              "      <td>Fair-GPTQ: Bias-Aware Quantization for Large Language Models</td>\n",
              "      <td>High memory demands of generative language models have drawn attention to\\nquantization, which reduces computational cost, memory usage, and latency by\\nmapping model weights to lower-precision integers. Approaches such as GPTQ\\neffectively minimize input-weight product errors during quantization; however,\\nrecent empirical studies show that they can increase biased outputs and degrade\\nperformance on fairness benchmarks, and it remains unclear which specific\\nweights cause this issue. In this work, we draw new links between quantization\\nand model fairness by adding explicit group-fairness constraints to the\\nquantization objective and introduce Fair-GPTQ, the first quantization method\\nexplicitly designed to reduce unfairness in large language models. The added\\nconstraints guide the learning of the rounding operation toward less-biased\\ntext generation for protected groups. Specifically, we focus on stereotype\\ngeneration involving occupational bias and discriminatory language spanning\\ngender, race, and religion. Fair-GPTQ has minimal impact on performance,\\npreserving at least 90% of baseline accuracy on zero-shot benchmarks, reduces\\nunfairness relative to a half-precision model, and retains the memory and speed\\nbenefits of 4-bit quantization. We also compare the performance of Fair-GPTQ\\nwith existing debiasing methods and find that it achieves performance on par\\nwith the iterative null-space projection debiasing approach on\\nracial-stereotype benchmarks. Overall, the results validate our theoretical\\nsolution to the quantization problem with a group-bias term, highlight its\\napplicability for reducing group bias at quantization time in generative\\nmodels, and demonstrate that our approach can further be used to analyze\\nchannel- and weight-level contributions to fairness during quantization.</td>\n",
              "      <td>[cs.CL]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2025-09-18 17:54:01+00:00</td>\n",
              "      <td>CausalPre: Scalable and Effective Data Pre-processing for Causal Fairness</td>\n",
              "      <td>Causal fairness in databases is crucial to preventing biased and inaccurate\\noutcomes in downstream tasks. While most prior work assumes a known causal\\nmodel, recent efforts relax this assumption by enforcing additional\\nconstraints. However, these approaches often fail to capture broader attribute\\nrelationships that are critical to maintaining utility. This raises a\\nfundamental question: Can we harness the benefits of causal reasoning to design\\nefficient and effective fairness solutions without relying on strong\\nassumptions about the underlying causal model? In this paper, we seek to answer\\nthis question by introducing CausalPre, a scalable and effective\\ncausality-guided data pre-processing framework that guarantees justifiable\\nfairness, a strong causal notion of fairness. CausalPre extracts causally fair\\nrelationships by reformulating the originally complex and computationally\\ninfeasible extraction task into a tailored distribution estimation problem. To\\nensure scalability, CausalPre adopts a carefully crafted variant of\\nlow-dimensional marginal factorization to approximate the joint distribution,\\ncomplemented by a heuristic algorithm that efficiently tackles the associated\\ncomputational challenge. Extensive experiments on benchmark datasets\\ndemonstrate that CausalPre is both effective and scalable, challenging the\\nconventional belief that achieving causal fairness requires trading off\\nrelationship coverage for relaxed model assumptions.</td>\n",
              "      <td>[cs.LG, cs.DB]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d4893d84-8cd7-4f79-a765-ab9d277fbc75')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d4893d84-8cd7-4f79-a765-ab9d277fbc75 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d4893d84-8cd7-4f79-a765-ab9d277fbc75');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-aaa4d135-dfd2-4b4c-815f-6c2cfb222261\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-aaa4d135-dfd2-4b4c-815f-6c2cfb222261')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-aaa4d135-dfd2-4b4c-815f-6c2cfb222261 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"published\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2025-09-18 17:54:01+00:00\",\n        \"max\": \"2025-09-18 17:59:58+00:00\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"2025-09-18 17:56:16+00:00\",\n          \"2025-09-18 17:59:51+00:00\",\n          \"2025-09-18 17:58:15+00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Fair-GPTQ: Bias-Aware Quantization for Large Language Models\",\n          \"Depth AnyEvent: A Cross-Modal Distillation Paradigm for Event-Based Monocular Depth Estimation\",\n          \"Evil Vizier: Vulnerabilities of LLM-Integrated XR Systems\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"High memory demands of generative language models have drawn attention to\\nquantization, which reduces computational cost, memory usage, and latency by\\nmapping model weights to lower-precision integers. Approaches such as GPTQ\\neffectively minimize input-weight product errors during quantization; however,\\nrecent empirical studies show that they can increase biased outputs and degrade\\nperformance on fairness benchmarks, and it remains unclear which specific\\nweights cause this issue. In this work, we draw new links between quantization\\nand model fairness by adding explicit group-fairness constraints to the\\nquantization objective and introduce Fair-GPTQ, the first quantization method\\nexplicitly designed to reduce unfairness in large language models. The added\\nconstraints guide the learning of the rounding operation toward less-biased\\ntext generation for protected groups. Specifically, we focus on stereotype\\ngeneration involving occupational bias and discriminatory language spanning\\ngender, race, and religion. Fair-GPTQ has minimal impact on performance,\\npreserving at least 90% of baseline accuracy on zero-shot benchmarks, reduces\\nunfairness relative to a half-precision model, and retains the memory and speed\\nbenefits of 4-bit quantization. We also compare the performance of Fair-GPTQ\\nwith existing debiasing methods and find that it achieves performance on par\\nwith the iterative null-space projection debiasing approach on\\nracial-stereotype benchmarks. Overall, the results validate our theoretical\\nsolution to the quantization problem with a group-bias term, highlight its\\napplicability for reducing group bias at quantization time in generative\\nmodels, and demonstrate that our approach can further be used to analyze\\nchannel- and weight-level contributions to fairness during quantization.\",\n          \"Event cameras capture sparse, high-temporal-resolution visual information,\\nmaking them particularly suitable for challenging environments with high-speed\\nmotion and strongly varying lighting conditions. However, the lack of large\\ndatasets with dense ground-truth depth annotations hinders learning-based\\nmonocular depth estimation from event data. To address this limitation, we\\npropose a cross-modal distillation paradigm to generate dense proxy labels\\nleveraging a Vision Foundation Model (VFM). Our strategy requires an event\\nstream spatially aligned with RGB frames, a simple setup even available\\noff-the-shelf, and exploits the robustness of large-scale VFMs. Additionally,\\nwe propose to adapt VFMs, either a vanilla one like Depth Anything v2 (DAv2),\\nor deriving from it a novel recurrent architecture to infer depth from\\nmonocular event cameras. We evaluate our approach with synthetic and real-world\\ndatasets, demonstrating that i) our cross-modal paradigm achieves competitive\\nperformance compared to fully supervised methods without requiring expensive\\ndepth annotations, and ii) our VFM-based models achieve state-of-the-art\\nperformance.\",\n          \"Extended reality (XR) applications increasingly integrate Large Language\\nModels (LLMs) to enhance user experience, scene understanding, and even\\ngenerate executable XR content, and are often called \\\"AI glasses\\\". Despite\\nthese potential benefits, the integrated XR-LLM pipeline makes XR applications\\nvulnerable to new forms of attacks. In this paper, we analyze LLM-Integated XR\\nsystems in the literature and in practice and categorize them along different\\ndimensions from a systems perspective. Building on this categorization, we\\nidentify a common threat model and demonstrate a series of proof-of-concept\\nattacks on multiple XR platforms that employ various LLM models (Meta Quest 3,\\nMeta Ray-Ban, Android, and Microsoft HoloLens 2 running Llama and GPT models).\\nAlthough these platforms each implement LLM integration differently, they share\\nvulnerabilities where an attacker can modify the public context surrounding a\\nlegitimate LLM query, resulting in erroneous visual or auditory feedback to\\nusers, thus compromising their safety or privacy, sowing confusion, or other\\nharmful effects. To defend against these threats, we discuss mitigation\\nstrategies and best practices for developers, including an initial defense\\nprototype, and call on the community to develop new protection mechanisms to\\nmitigate these risks.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"categories\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Example abstract from API\n",
        "abstract = df['abstract'][0]\n",
        "\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Summarization\n",
        "summarization_result = summarizer(abstract)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226,
          "referenced_widgets": [
            "711594057a1f459e9d28c6151f032d82",
            "ac9674e7d2ff49189cc63677a8143f1b",
            "5a2fb26cc8aa44c484fb771b4937d8bf",
            "38a4befa74b9466384279f834733967c",
            "8fb4374a49094061b733c0333f68973a",
            "e63e848d095a404f87fcec8864a152d4",
            "7cb7f80c79b2443cac86551a21e8749c",
            "d3dd730846284b1583a1f3c99b0c6bf2",
            "5b716560bf594f36b66f279e18db3f75",
            "5bfb2efbfd9946548a0dbbb57a160129",
            "5aebd036a2ea46b09b8775e784eff58a",
            "b52404fddd2c43ca8b0a53ab6f76fc9d",
            "bb493ad43d62483e9086dce4e796750f",
            "8f7165f2972442b7860ba529da035def",
            "16a42d9444ef4bccb86023addc65cd5e",
            "400a0dd53aa647e1a2aae8718c978835",
            "80d0e4a843454ef3aba1431082591beb",
            "579975f53205416e90d4550f25b5bd44",
            "b2b5f6771fb94656b61fba3cae3e44a2",
            "3bea5ab4aaec4458907a695198b6c19e",
            "229cd375ad9f43db90b9ef7a96c1236f",
            "e12dd8e7d74549698ea1cdd696ea5fcf",
            "c08e29393ae742e29ac0fabc8a8d0525",
            "2cf776fc827343788361c9b819f356dc",
            "19f515776b2c4eeaafee6b1972f8b224",
            "1903b25915a945989432326ffe0e9a07",
            "1e7d08f863364d4d92323dbdfec5cd91",
            "bf9740df6c9d48d4b19b93d2dd94fad2",
            "dd607947737d4f069e9c427159a280a6",
            "5e896854670549a2ab1a79abb3900454",
            "efa282fde1e24b26929b62f1480d9df2",
            "f28da7ace0e64bf98c2db8f711b3a7f3",
            "ef77d775102d4b4cb97e0afbd85accee",
            "ddfeccf187d14433ab563015479b15d9",
            "a794abc381664c49b15741aed5f51611",
            "9890e858ab6a442aabd2a7d0b871d3cb",
            "5d5282f6b0814566a5cd7388e706dc2e",
            "e8a1461cf2ca456bb9edbede7e53b195",
            "4d1f8fc696684c609a61ef793c5dd91b",
            "91639325310a487e91d4bae42205567a",
            "91d3a3afae444f0495d7af992b96075e",
            "85706d135bc04d4aa9ca84e6a4b82cf1",
            "469b180f2cc045abac50009f2b086303",
            "99a4fefb75ec4cf48e7baccb83d85865",
            "85dfdeb1c1294f729abc815e6e42d385",
            "68a9ab0a452340a0b8c1e6ddfe4c57f7",
            "2589256763f14f24978ecefa2a558b9f",
            "c6f6c6bb491f472b9d0fccd4dad8e627",
            "8c88abdb98ff4e0f9be3ead9041e968a",
            "4c8c512ed9324d50b0e43a6544dad417",
            "da81431298f548ba989715cd1843a495",
            "de105f35a82a470797ef21fac3e25b05",
            "9bb17520991b474295fdc5f2a1060699",
            "7fe3aedce40e41d39f47a68db4fee1a8",
            "54082835f80f44a5b4bb063efa19a712",
            "ae731d176ecd41e49a1fe53065472d7d",
            "c0c5e29e0e184086b5223c21e76909aa",
            "4c33fdb47e854556b6ae7539d06be0cb",
            "8910b957f7ab4910b157b7d173bb4048",
            "c0518b5fd09a4549947018e93304e8f0",
            "fa6429fcb8b943cc91b95095bb25a20c",
            "ab1eea50d1a6489297fefb4307aaa2af",
            "4581c7a11e774d6582a06bda44771a9a",
            "45a2fce2f44b4d88a974aaab8cebfb1b",
            "285d6bcd4569411bbbf01985841c4bb9",
            "5dcbb27d2a22459198dbeceb1500fa75"
          ]
        },
        "id": "F7OauAnTfnRV",
        "outputId": "08d97c4e-caeb-497b-ed15-e16857ba8d9d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "711594057a1f459e9d28c6151f032d82"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b52404fddd2c43ca8b0a53ab6f76fc9d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c08e29393ae742e29ac0fabc8a8d0525"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ddfeccf187d14433ab563015479b15d9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "85dfdeb1c1294f729abc815e6e42d385"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ae731d176ecd41e49a1fe53065472d7d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summarization_result[0]['summary_text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "-u3tSueXjmGy",
        "outputId": "6f3475ac-e4e8-452c-9c5b-0d2271b60e9c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Medical Vision-Language Models (Med-VLMs) have demonstrated remarkable performance across diverse medical imaging tasks. CalibPrompt is the first framework to calibrate Med-VLM during prompt tuning. It consistentlyimproves calibration without drastically affecting clean accuracy. The code is available at https://github.com/iabh1shekbasu/CalibPromPT.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}